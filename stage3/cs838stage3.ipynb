{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS838 Project Stage3\n",
    "\n",
    "Yuying Chen\n",
    "Maria Kamenetsky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!which -a python\n",
    "#!echo $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import modules\n",
    "import csv\n",
    "import py_entitymatching as em\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import shutil\n",
    "import collections\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import scipy\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the original two movie dataset that we selected don't have too many overlaps, we changed one of them to another movie dataset. The first dataset is from kaggle.com. The second dataset is from movielens.org.\n",
    "\n",
    "We added an id column for both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = em.read_csv_metadata('../../DataforProject/movie1_stage3.csv', key='id',encoding='mac_roman')\n",
    "B = em.read_csv_metadata('../../DataforProject/movie2_stage3.csv', key='movieId',encoding='mac_roman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Number of tuples in A: ' + str(len(A)))\n",
    "print('Number of tuples in B: ' + str(len(B)))\n",
    "print('Number of tuples in A x B: ' + str(len(A)*len(B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "B.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "em.get_key(A), em.get_key(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the tables are very large let's downsample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A1, B1 = em.down_sample(A,B, 200,1, show_progress=True)\n",
    "len(A1), len(B1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Block Tables To Get Candidate Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block on attribute 'year' using an overlap blocker since the same movie must have the same year. The output attributes of table A is limited to id, title, year and genres to match the information provided by table B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create overlap blocker\n",
    "ob = em.OverlapBlocker()\n",
    "# Block tables using 'year' attribute \n",
    "#C = ob.block_tables(sample_A, sample_B, 'title_year', 'year', \n",
    " #                   l_output_attrs=['id', 'movie_title', 'title_year', 'genres'], \n",
    "#                  r_output_attrs=['movieId', 'title', 'year', 'genres'],\n",
    "   #                 overlap_size=1, show_progress=True\n",
    "                    #)\n",
    "C = ob.block_tables(A, B, 'year', 'year',\n",
    "                   l_output_attrs=['id', 'title', 'year', 'genres'],\n",
    "                   r_output_attrs=['movieId', 'title','year', 'genres'],\n",
    "                   overlap_size=1, show_progress=True)\n",
    "em.to_csv_metadata(C, './candidate_pairs.csv') #I really like this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match tuple pairs in candidate set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Pre-Determined Golden Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly sampled 500 tuple pairs for labeling. There are 201 matches and 299 mismatches. **gold.csv** is our gold dataset of perfect matches that we have determined prior to this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#G = pd.read_csv(\"../../DataforProject/gold.csv\", encoding='mac_roman')\n",
    "#len(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G = em.read_csv_metadata('../../DataforProject/gold.csv', \n",
    "                         key='_id',\n",
    "                            ltable=A, rtable=B, \n",
    "                        fk_ltable='ltable_id', fk_rtable='rtable_movieId',\n",
    "                         encoding='mac_roman')\n",
    "len(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#G = em.label_table(G, 'gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "We note here, that based on what we saw in the creation of the golden data, we establish a *correct* match as:\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Splitting Labeled Data into Development and Evaluation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next want to split our data into a development set, where we will iteratively train and test various classifiers on the data. The evaluation set will be our final hold out set which we will test last and only once, so that we avoid overfitting. \n",
    "\n",
    "Let *I* be the development set and *J* be the evaluation set. We split the data into a 70-30 split for development-evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IJ = em.split_train_test(G, train_proportion=0.7, random_state=0)\n",
    "I = IJ['train']\n",
    "J = IJ['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Package note**: the above split_train_test did not work if I imported G using pandas read_csv, even with encoding. I had to import it using\n",
    "    ```em.read_csv_metadata()```, which is a little confusing to first-time user since it's a csv, but not necessarily metadata.\n",
    "    Is there another function that would import metadata and do something with it separately? Or a way to interface with ```pd.read_csv()``` so that  you can read in a csv, clean it and work with it, and then reimport with ```em.read_csv_metadata()``` somehow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Best Learning-Based Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first go through and set our various machine-learning matchers that we will cycle through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a set of ML-matchers\n",
    "dt = em.DTMatcher(name='DecisionTree', random_state=0)\n",
    "svm = em.SVMMatcher(name='SVM', random_state=0)\n",
    "rf = em.RFMatcher(name='RF', random_state=0)\n",
    "lg = em.LogRegMatcher(name='LogReg', random_state=0)\n",
    "ln = em.LinRegMatcher(name='LinReg')\n",
    "nb = em.NBMatcher(name='NaiveBayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In exploring the **py_entitymatching**, we first start with using the automatically generated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the auto features\n",
    "feature_table = em.get_features_for_matching(A,B)\n",
    "\n",
    "feature_table.feature_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Package Note**: The above failed with error: `KeyError: \"['feature_name' 'left_attribute' 'right_attribute' 'left_attr_tokenizer'\\n 'right_attr_tokenizer' 'simfunction' 'function' 'function_source'\\n 'is_auto_generated'] not in index\"`\n",
    "\n",
    "\n",
    "Also, the ```get_features_for_matching()``` doesn't work if A and B do not have any common features. Which totally makes sense, but the error message doesn't make this clear for sleepy people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert to feature vectors\n",
    "H = em.extract_feature_vecs(I,\n",
    "                           feature_table=feature_table,\n",
    "                           attrs_after='label',\n",
    "                           show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting best ML Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Now we want to select the best matcher. We use k-fold cross validation. We will use 10 folds here and use precision, recall, and F1 to select the best matcher.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
